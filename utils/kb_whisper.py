"""
KB-Whisper integration fÃ¶r lokal svensk transkribering
AnvÃ¤nder KBLab's Whisper-modeller som Ã¤r trÃ¤nade pÃ¥ 50,000 timmar svensk data
Ger 47% bÃ¤ttre resultat Ã¤n OpenAI Whisper fÃ¶r svenska
"""

import os
import streamlit as st
import torch
from typing import Optional, Literal

# Global cache fÃ¶r modellen
_kb_whisper_model = None
_kb_whisper_processor = None
_kb_whisper_pipe = None

# TillgÃ¤ngliga modeller frÃ¥n KBLab
KB_WHISPER_MODELS = {
    "large": "KBLab/kb-whisper-large",      # 2B parametrar - BÃ¤st kvalitet
    "medium": "KBLab/kb-whisper-medium",    # 0.8B parametrar - Balans
    "small": "KBLab/kb-whisper-small",      # 0.3B parametrar - Snabbare
    "base": "KBLab/kb-whisper-base",        # 99M parametrar - Mycket snabb
    "tiny": "KBLab/kb-whisper-tiny"         # 57M parametrar - Snabbast
}

def get_kb_whisper_model_size():
    """HÃ¤mta vald modellstorlek frÃ¥n environment eller anvÃ¤nd default"""
    return os.getenv('KB_WHISPER_MODEL', 'medium')

def get_kb_whisper_model_id():
    """HÃ¤mta model ID baserat pÃ¥ vald storlek"""
    size = get_kb_whisper_model_size()
    return KB_WHISPER_MODELS.get(size, KB_WHISPER_MODELS['medium'])

def get_transcription_style():
    """
    HÃ¤mta transkriberingsstil frÃ¥n environment
    - default: Standard transkribering
    - subtitle: Mer komprimerad stil
    - strict: Mer verbatim-lik
    """
    return os.getenv('KB_WHISPER_STYLE', 'default')

def load_kb_whisper_model():
    """
    Ladda KB-Whisper modellen i minnet (caching)
    Returnerar (model, processor, pipeline) eller None vid fel
    """
    global _kb_whisper_model, _kb_whisper_processor, _kb_whisper_pipe

    # Om modellen redan Ã¤r laddad, returnera den
    if _kb_whisper_pipe is not None:
        return _kb_whisper_model, _kb_whisper_processor, _kb_whisper_pipe

    try:
        from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline

        # VÃ¤lj device (GPU om tillgÃ¤ngligt, annars CPU)
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

        model_id = get_kb_whisper_model_id()
        style = get_transcription_style()

        st.info(f"ðŸ”„ Laddar KB-Whisper modell: {model_id} (stil: {style})")
        st.info(f"ðŸ“Š AnvÃ¤nder: {device.upper()}")

        # Ladda modellen med rÃ¤tt revision om subtitle eller strict
        model_kwargs = {
            "torch_dtype": torch_dtype,
            "use_safetensors": True,
            "cache_dir": "cache"
        }

        if style in ["subtitle", "strict"]:
            model_kwargs["revision"] = style

        _kb_whisper_model = AutoModelForSpeechSeq2Seq.from_pretrained(
            model_id,
            **model_kwargs
        )
        _kb_whisper_model.to(device)

        # Ladda processor
        _kb_whisper_processor = AutoProcessor.from_pretrained(model_id)

        # Skapa pipeline
        _kb_whisper_pipe = pipeline(
            "automatic-speech-recognition",
            model=_kb_whisper_model,
            tokenizer=_kb_whisper_processor.tokenizer,
            feature_extractor=_kb_whisper_processor.feature_extractor,
            torch_dtype=torch_dtype,
            device=device,
        )

        st.success(f"âœ… KB-Whisper modell laddad: {model_id}")

        return _kb_whisper_model, _kb_whisper_processor, _kb_whisper_pipe

    except Exception as e:
        st.error(f"âŒ Kunde inte ladda KB-Whisper modell: {e}")
        st.warning("ðŸ’¡ Kontrollera att transformers och torch Ã¤r installerade.")
        return None, None, None

def transcribe_with_kb_whisper(audio_file_path: str) -> Optional[str]:
    """
    Transkribera en ljudfil med KB-Whisper

    Args:
        audio_file_path: SÃ¶kvÃ¤g till ljudfil

    Returns:
        Transkribering som strÃ¤ng eller None vid fel
    """
    try:
        # Ladda modellen (cachas automatiskt)
        model, processor, pipe = load_kb_whisper_model()

        if pipe is None:
            return None

        # Transkribera med KB-Whisper
        # chunk_length_s=30 fÃ¶r att hantera lÃ¥nga filer effektivt
        generate_kwargs = {
            "task": "transcribe",
            "language": "sv"
        }

        with st.spinner(f"ðŸŽ¤ Transkriberar med KB-Whisper..."):
            result = pipe(
                audio_file_path,
                chunk_length_s=30,
                generate_kwargs=generate_kwargs
            )

        # Resultat Ã¤r en dict med "text" key
        transcription = result.get("text", "")

        if transcription:
            st.success("âœ… KB-Whisper transkribering klar!")
            return transcription
        else:
            st.error("âŒ Ingen transkribering genererades")
            return None

    except Exception as e:
        st.error(f"âŒ Fel vid KB-Whisper transkribering: {e}")
        return None

def unload_kb_whisper_model():
    """
    FrigÃ¶r minne genom att ta bort modellen frÃ¥n RAM/VRAM
    AnvÃ¤nd detta om du vill spara minne nÃ¤r modellen inte anvÃ¤nds
    """
    global _kb_whisper_model, _kb_whisper_processor, _kb_whisper_pipe

    if _kb_whisper_model is not None:
        del _kb_whisper_model
        del _kb_whisper_processor
        del _kb_whisper_pipe

        _kb_whisper_model = None
        _kb_whisper_processor = None
        _kb_whisper_pipe = None

        # Rensa CUDA cache om GPU anvÃ¤nds
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        st.info("ðŸ—‘ï¸ KB-Whisper modell borttagen frÃ¥n minnet")

def is_kb_whisper_available() -> bool:
    """
    Kontrollera om KB-Whisper dependencies Ã¤r installerade
    """
    try:
        import transformers
        import torch
        import accelerate
        import librosa
        import soundfile
        return True
    except ImportError:
        return False

def get_kb_whisper_info() -> dict:
    """
    HÃ¤mta information om KB-Whisper konfigurationen
    """
    return {
        "available": is_kb_whisper_available(),
        "model_size": get_kb_whisper_model_size(),
        "model_id": get_kb_whisper_model_id(),
        "style": get_transcription_style(),
        "cuda_available": torch.cuda.is_available() if is_kb_whisper_available() else False,
        "loaded": _kb_whisper_pipe is not None
    }
